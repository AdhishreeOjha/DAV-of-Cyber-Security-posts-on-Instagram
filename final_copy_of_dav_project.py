# -*- coding: utf-8 -*-
"""Final Copy of DAV Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_TxnyYEbcQzYuPVFtvVY-KYr7nYOZItI

# **Exploratory Data Analysis of Social Media posts on Instagram with Hashtag Cybersecurity**

**Context**

Every coin has two sides and so is the case in the world of technology. With the advances in the field of computers, the world has been promised for an easy, faster and better life. But as said earlier, with the good comes the bad and so with this advancement comes the threats to the data and thus in the fastly growing cyber world, cyber security becomes one of the most essential and key factor in our day to day life.Cyber Security is now the ‘need of the hour’ as it provides with the ways to combat threats and protect critical systems and the sensitive information from the various digital attacks.Apart from making efforts to combat the attacks it also becomes to be aware about each and every aspect of it as always said prevention is better than cure.And thus more and more people should be aware of it and in creating awareness among people we could use the social media platforms among which the most trendy is Instagram nowadays.So we will we analysing people's awareness about cyber security on this particular social media platform and will see how much the world has become technosavy in actually means.

**Objective**

> Instagram post analytics can be the best way to understand who your audience is, when they’re most active, and what type of content they engage with the most and thus the analysis of the cyber security related post help us to analyse and visualize people's engagement and interest in this particular field.It also helps us to understand the differnt trendy posts at different time period.Apart from this it will help to undersatnd how much concerened, the people are about the cyber world and upto what extent more awareness has to be spread among the people.

**Data desription**

> 


The data contains the different data related to the cyber security related posts on Instagram. The detailed data dictionary is given below.

**Data dictionary**

> 
1. link : Displays the link of the post.
> 
2. author :This tells the of the post who has posted.
> 
3. img_link : Displays the link of the image in the post.
> 
4. date_of_upload  :Displays the date of the upload of the post.
> 
5. likes : Displays the number of likes to a particulat post.
> 
6. timestamp : Displays the time at which the post was scrapped.
> 
7. caption : Displays the caption under which the post was posted.

**Importing libraries needed for our project:-**
---




1. langdetect - Library used for language detection (65 languages available)
1. numpy: for performing mathematical operations on the arrays
1. pandas: for statistical analysis of the data
2. matplotlib: for visualizing the data
3. seaborn: for visualizing the data
3. seaborn: for visualizing random distributions
4. %matplotlib inline: for plotting graph in the current shell only and not on separate window
5. re: for regular expressions
6. wordcloud: used to design graphic for wordcloud
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install langdetect
!pip install vaderSentiment
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from wordcloud import WordCloud, STOPWORDS
# %matplotlib inline

"""**Instagram Scrapper:**
---

Scrapper was run on a local Windows Machine with Chrome Driver and selenium python package. Data was collected each day for a span of 3 weeks.

```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import pandas as pd
from datetime import datetime 
import csv
import time
options = Options()
# PROXY = "11.456.448.110"
options.page_load_strategy = 'normal'
# options.add_argument('--proxy-server=%s' % PROXY)
driver = webdriver.Chrome(options=options)
driver.get('https://instagram.com')

for i in range(190,8000):
    try:
        df = pd.read_csv("C:\Projects\InstagramScrapper\\testlist02_captions.csv")
        driver.get(df['links'][i])
        driver.implicitly_wait(5)
        df['image'][i] = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/div[1]/article/div/div[1]/div/div/div/div[1]/img').get_attribute("src")
        df['upload_time'][i] = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/div[1]/article/div/div[2]/div/div[2]/div[2]/div/div/a/div/time').text
        df['likes'][i] = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/div[1]/article/div/div[2]/div/div[2]/section[2]/div/div/div/a/div/span').text
        df['authors'][i] = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/div[1]/article/div/div[2]/div/div[2]/div[1]/ul/div/li/div/div/div[2]/h2/div/span/a').text
        df['timestamp'][i] = datetime.now()
        df['captions'][i] = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[1]/div/div/div/div[1]/div[1]/div[2]/section/main/div[1]/div[1]/article/div/div[2]/div/div[2]/div[1]/ul/div/li/div/div/div[2]/div[1]/span').text
        df.to_csv("C:\Projects\InstagramScrapper\\testlist02_captions.csv", index = False)
    except:
        continue

for i in range(0,100):
    driver.implicitly_wait(5)
    f = open("C:\\Projects\\InstagramScrapper\\testlist02.txt", "a")
    f.write(a1 + ',' + a2 +','+ a3 +',' + a4 +',' +','+a5 +',' +a6 + '\n')
    f.close()
    driver.find_element(By.XPATH, '/html/body/div[2]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[1]/div/div/div[2]/button/div/span').send_keys("driver" + Keys.ENTER)

count = 0
for i in range(0,300000):
    # time.sleep(2)
    a0 = driver.current_url
    try:
        a1 = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[2]/div/article/div/div[2]/div/div/div[1]/div/header/div[2]/div[1]/div[1]/div/div/div/span/a').text
    except:
        a1 = ''
    try:
        a2 = driver.find_element(By.XPATH,'//*[@id="mount_0_0_r1"]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[2]/div/article/div/div[1]/div/div/div/div[1]/img').get_attribute("src")
    except:
        a2 = ''
    try:
        a3 = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[2]/div/article/div/div[2]/div/div/div[2]/div[2]/div/div/a/div/time').get_attribute("title")
    except:
        a3 = ''
    try:
        a4 = driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[2]/div/article/div/div[2]/div/div/div[2]/section[2]/div/div/div/a/div').text
    except:
        a4 = ''
    a5 = str(datetime.now())
    try:
        a6 = str(driver.find_element(By.XPATH,'/html/body/div[2]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[2]/div/article/div/div[2]/div/div/div[2]/div[1]/ul/div/li/div/div/div[2]/div[1]/span').text)
    except:
        a6 = ''
    with open("C:\\Projects\\InstagramScrapper\\testlist03_captions.csv", "a", encoding='utf-8') as f:
        writer = csv.writer(f)
        data = [a0,a1,a2,a3,a4,a5,a6]
        writer.writerow(data)
    f.close()
    count=count+1
    print("Posts Scanned = {0}".format(count))
    driver.find_element(By.XPATH, '//*[@id="mount_0_0_r1"]/div/div/div/div[2]/div/div/div[1]/div/div[3]/div/div/div/div/div[1]/div/div/div[2]/button').send_keys("driver" + Keys.ENTER)

```

# **Understanding and Analysis of Data**
---
"""

df=pd.read_csv('/content/testlist03_captions - Copy(2)1.csv')
# df=pd.read_csv('/content/testlist03_captions - Copy(2)1.csv.xls')
df.head()

"""**Observation:-** Dataframe has a total of 7 columns and the dataset is about the cyber security related posts on Instagram and it's reach on the particular social media platform.

**Question 1-** Describe the number of rows and columns in the dataset.
---
"""

print('The dataset has',df.shape[0],'rows and',df.shape[1],'columns')

"""**Question 2-** What are the datatypes of the different columns in the dataset?
---
"""

df.info()

"""**Observations:-**In the dataset all the columns have been converted to the same data type,i.e,we have all 7 attributes of type object.We can also see that there are a total of 48019 entries ranging from 0 to 48018.

**Change datatypes to proper format**
---
"""

df['date_of_upload'] = pd.to_datetime(df['date_of_upload'])
df.info()

"""**Observation:-**data type of date_of_upload changed to datetime64[ns] and that of likes changed to float64.

**Question 3:- Are there any missing values in the data?**
---
"""

df.isnull().sum()

"""**Observation:-**We have missing values in some of the attributes like in author,img_link,date_of_upload,likes,caption.

**Question 4:-Show the statstical summary of the dataset.**
---
"""

df.describe()

"""Observations:-

Total count of rows- 48019 
Individual column's count of rows-
*   48019 for link
*   43186 for author
*   33410 for img_link
*   47974 for date_of_upload
*   32842 for likes
*   48019 for timestamp
*   47956 for caption

Numebr of unique values in the columns-
*   32573 in link
*   7995 in author
*   29930 in img_link
*   58 in date_of_upload
*   1403 in likes
*   25755 in timestamp
*   20193 in caption

The top gives the highest counted value of the specific columns as shown in the table.

freq(number of times the most frequent element occured)-
*   8 times in link
*   275 times in author
*   6 times in img_link
*   5814 times in date_of_upload
*   7317 times in likes
*   8 times in timestamp
*   448 times in caption

For the post in the dataset-
*   Date of first post:-2019-02-11 00:00:00
*   Date of last post:-2023-01-12 00:00:00

PS: Table shows summary only for likes as its data type is float.

**Question 5:-Remove duplicate posts in the dataset**
---
"""

#Q5 - Remove duplicate posts in the dataset
df1 = df.copy()
df1.drop(['timestamp', 'img_link'], axis=1)
df1 = df1.drop_duplicates(subset=['link'], keep='last')
df1.describe()

"""**Observation:-**
Dropping all the duplicate entries of each individual columns.
Now new unique entries count is:
*   link-32573 entries	
*   author-7983 entries(because one author can have different posts)
*   img_link-22997 entries
*   date_of_upload-58 entries (because many posts can be posted on the same day)
*   likes-1193 entries(a post can have many likes)
*   timestamp-21588 entries(time could be same for various posts posted)
*   caption-20173 entries(caption could be same for various posts posted )

Even the value of freq,top,first and last changes as shown in the table.

**Question 6:-Find top 5 authors with maximum posts in the given time interval.**
---
"""

df2 = df1.pivot_table(index = ['author'], aggfunc ='size')
df2.sort_values(ascending=False).head()

"""**Observations:-**The top five authors according to maximum posts related to cyber security are:
*   devi_lal_kalal with 147 posts overall
*   emwpresswire with 127 posts overall
*   lewis_cyberpunk  with 127 posts overall
*   trcak.hackies with 125 posts overall
*   54_deck with 122 posts overall

So we can infer that devi_lal_kalal has posted the maximum number of cyber related posts.

**Question 7:-Plot the bar graph for date of upload vs number of posts**
---
"""

#Sorted
df3 = df1.pivot_table(index = ['date_of_upload'], aggfunc ='size')
df3 = df3.sort_values(ascending=False)
ax = df3.plot.barh(figsize=(20,20))
y = df1['date_of_upload'].value_counts(ascending=False)
for i, v in enumerate(y):
    ax.text(v + 10, i, str(v), color='black', fontweight='bold', fontsize=14, ha='left', va='center')

#Unsorted
df3 = df1.pivot_table(index = ['date_of_upload'], aggfunc ='size')
ax = df3.plot.bar(figsize=(20,20))

"""**Observation:-**The graph shows the bargraph with date of upload on the y-axis and the number of posts on the x-axis.It shows the number of posts in the ascending order with the maximum number of post being shared on 2023-01-11 with a count of 2494.

**Question 8:-Plot the graph for number of likes vs post**
---
"""

df4 = df1.copy()
df4 = df4.drop(['timestamp', 'img_link', 'caption', 'author', 'date_of_upload'], axis=1)
df4 = df4.dropna()
df4.sort_values( by='likes' ,ascending=False).head()
ax = df4.plot.bar()

"""Observations:-The graphs shows the number of likes for each post with the maximum likes of 6.67E+10 likes for a post.

**Question 9:-Display top 1000 posts with maximum likes**
---
"""

df1.sort_values( by='likes' ,ascending=False).head(1000)

"""Obsevation:-The table shows the number of likes for the top 1000 posts (sorted in descending order of likes likes.)

**Question 10:-Plot the graph of author vs posts**
---
"""

df2 = df1.pivot_table(index = ['author'], aggfunc ='size')
df2 = df2.sort_values(ascending=False)
df2

ax = df2.plot.barh(figsize=(200,200))
y = df1['author'].value_counts(ascending=False)
for i, v in enumerate(y):
    ax.text(v, i, str(v), color='black', fontweight='bold', fontsize=5, ha='left', va='center')

"""Observations:-The graph shows the number of posts  made by an author with the maximum value being 147 posts.

**Question 11:-Plot boxplot and histplot for likes vs count**
---
"""

sns.histplot(data = df4, x='likes',bins = 4,stat = 'count',kde = True)
plt.show()

sns.boxplot(data = df4, x='likes')
plt.show();

"""**Obsevations:-**The graph shows that the count of posts with less likes are large in count whereas that of more likes are less in count. This is an example of a skewed graph.

**Question 12:-Analyze categorize captions by languages and show count**
---
"""

# Caption analysis - categorize captions by languages and show count
df5 = df1.copy()
df5 = df5.drop(['author', 'img_link', 'date_of_upload', 'timestamp', 'likes'], axis = 1)
df5 = df5.dropna()
from langdetect import detect

def detect_lang(x):
  try:
    return detect(x)
  except:
    return ''

df5['language'] = df5['caption'].apply(lambda text: detect_lang(text))
df5

df6 = df5.pivot_table(index = ['language'], aggfunc ='size')
df6 = df6.sort_values(ascending=False)
df6

ax = df6.plot.barh(figsize=(20,30))
y = df5['language'].value_counts(ascending=False)
for i, v in enumerate(y):
    ax.text(v, i, str(v), color='black', fontweight='bold', fontsize=10, ha='left', va='center')

df6.plot.pie(legend='True', figsize=(20,20))

"""Observation:-This graph shows the count of the languages used in the post according to the table achieved above.

**Question 13:- Find posts with identical captions by same author**
---
"""

#Caption analysis - find posts with identical captions by same author
df7 = df1.pivot_table(index = ['caption', 'author'], aggfunc ='size')
df7 = df7.sort_values(ascending=False)
df7

"""**Observations:-**This shows all the captions used frequently by an author in all his posts made.

**Question 14:-Word cloud of the #hashtags used in the captions**
---
"""

#Word cloud of the #hashtags used in the captions

i=0
text1 = df5['caption']
# text2 = df5['cat']
hashtag_list = []
hashtag_list1 = {}
i=-1
for text in text1:
    i+=1
    sublist = []
    for word in text.split():
        if word[0] == '#':
            sublist.append(word)
    hashtag_list.append(sublist)

comment_words = str(hashtag_list)
stopwords = set(STOPWORDS)
wordcloud = WordCloud( random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(comment_words)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""**Observation:** Above is the final Word Cloud generated. Font size is directly proportional to the frequency in the posts

**Question 15: Find the trend with which top 5 authors have posted in the entire time interval**
---
"""

#Find top 5 authors with maximum posts
df8 = df1.pivot_table(index = ['author'], aggfunc ='size')
df8 = df8.sort_values(ascending=False).head()
df8

df9 = df1.copy()
df9 = df9.drop(['img_link', 'timestamp', 'likes'], axis = 1)
df9 = df9.dropna()

df90 = df9[df9['author'].str.contains('devi_lal_kalal')]
df91 = df9[df9['author'].str.contains('emwpresswire')]
df92 = df9[df9['author'].str.contains('lewis_cyberpunk')]
df93 = df9[df9['author'].str.contains('trcak.hackies')]
df94 = df9[df9['author'].str.contains('54_deck')]

df10 = df90.pivot_table(index = ['date_of_upload'], aggfunc ='size')
df11 = df91.pivot_table(index = ['date_of_upload'], aggfunc ='size')
df12 = df92.pivot_table(index = ['date_of_upload'], aggfunc ='size')
df13 = df93.pivot_table(index = ['date_of_upload'], aggfunc ='size')
df14 = df94.pivot_table(index = ['date_of_upload'], aggfunc ='size')

ax = df10.plot.line()
ax = df11.plot.line()
ax = df12.plot.line()
ax = df13.plot.line()
ax = df14.plot.line()

"""Observation: Maximum posts have been uploaded during the time interval between christmas and new year. One more peak is also observed at around 7th-9th Jan 2023

**Question 16: Keyword Search in Captions**
---
"""

df15 = df9[df9['caption'].str.contains('certificate')]
df15

df16 = df9[df9['caption'].str.contains('course')]
df16

df17 = df9[df9['caption'].str.contains('admission')]
df17

df18 = df9[df9['caption'].str.contains('conference')]
df18

df19 = df9[df9['caption'].str.contains('AIIMS')]
df19

"""Observation:"""

df20 = df9[df9['caption'].str.contains('awareness')]
df20

otherlen = len(df9) - (len(df20) + len(df19) + len(df18) +  len(df17) +  len(df16) +  len(df15))
y = np.array([len(df20), len(df19), len(df18), len(df17), len(df16), len(df15), otherlen])
mylabels = ["Awareness", "AIIMS", "Conference", "Admissions", "Courses", "Certificates", "Others"]

plt.pie(y, labels = mylabels)
plt.legend()

"""**Question 17: Sentiment Analysis of Captions**
---
"""

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sentiment = SentimentIntensityAnalyzer()

def detect_senti_score(x):
  try:
    return sentiment.polarity_scores(x)
  except:
    return ''

df5['Polarity Score'] = df5['caption'].apply(lambda text: detect_senti_score(text))
df5

df21 = df5['Polarity Score']
df21

"""Observation: Seniment Analysis has been performed successfully

**C. Conclusion:**
---
1. New dataset has been created using a self-developed Instagram scrapper which has scrapped 48019 posts over time interval of 3 weeks. Data has been preprocessed for dealing with missing values and duplicate values.
2. Data summary has been presented.
3. Emphasis has been given on Univariate Exploratory Data Analysis
4. Out of the Entire dataset, less than 25% posts have useful data. Others are just fake news or spam posts.
"""